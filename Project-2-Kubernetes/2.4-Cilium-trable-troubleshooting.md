## Troubleshooting Cilium on NSX: A Real-World Deep Dive as an SRE / Platform Engineer

Over the past two days, I investigated and resolved a complex networking issue involving Cilium, Kubernetes, and an underlying VMware NSX virtualized network.
This post walks through my troubleshooting approach, the hypotheses I tested, the tools I used, and how I finally identified and fixed the root cause.

### Initial Symptoms

I deployed Cilium using the same configuration I usually rely on—nothing special, no exotic settings.
However, this time Cilium pods were stuck, showing errors, and the overall Cilium status was red.

Running:

```bash
kubectl -n kube-system exec -it ds/cilium -- cilium-health status
```
revealed that the cluster inter-node network was broken.
The health checks showed failures in connectivity between nodes.

### Step 1 — Packet Tracing Between Nodes

To investigate inter-node communication, I used tcpdump on both nodes to inspect VXLAN traffic (UDP 8472):

**On Node 1**
```bash
sudo tcpdump -i ens180 -n -vvv udp port 8472
```

**On Node 2**
```bash
sudo tcpdump -i ens181 -n -vvv 'udp dst port 8472'
```

I could clearly see packets leaving Node 1, but nothing was received on Node 2.

At this point, my suspicion shifted to the virtual network layer.

### Step 2 — Suspecting NSX VXLAN Decapsulation

Since the VMs were running inside VMware NSX, I suspected VXLAN packets might be decapsulated or filtered by NSX before reaching the other node.

To validate this idea, I changed the VXLAN port in Cilium’s configuration and redeployed.

After applying the new configuration:

```bash
cilium-health status
```

* ICMP checks were now working
* HTTP health checks were still failing

So the underlying network path was partially fixed, but not completely.

### Step 3 — More Tracing: ICMP Works, HTTP Doesn’t
I ran tcpdump again on both nodes.
This time:

* ICMP packets were visible
* HTTP packets were missing completely

This pointed to packet fragmentation or MTU issues.